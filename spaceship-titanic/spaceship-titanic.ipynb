{"cells":[{"cell_type":"markdown","metadata":{},"source":["# ðŸš€ Spaceship Titanic\n","\n","## Description:\n","Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n","\n","The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n","\n","While rounding Alpha Centauri en route to its first destinationâ€”the torrid 55 Cancri Eâ€”the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n","\n","To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceshipâ€™s damaged computer system.\n","\n","Help save them and change history!"]},{"cell_type":"markdown","metadata":{},"source":["## âš™ï¸ Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install --upgrade --force-reinstall git+https://github.com/FlorianTeich/kaggle_tools"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import kaggle_tools\n","\n","kaggle_tools.get_data_path()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2023-11-26T11:31:56.317455Z","iopub.status.busy":"2023-11-26T11:31:56.317025Z","iopub.status.idle":"2023-11-26T11:32:20.669252Z","shell.execute_reply":"2023-11-26T11:32:20.668027Z","shell.execute_reply.started":"2023-11-26T11:31:56.317422Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!pip install sentence-transformers pygwalker kedro mlflow kedro-viz pyspark==3.3.0 xgboost==2.0.2 hyperopt"]},{"cell_type":"markdown","metadata":{},"source":["## ðŸ“Š Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kaggle_tools.get_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T11:51:47.927362Z","iopub.status.busy":"2023-11-26T11:51:47.92697Z","iopub.status.idle":"2023-11-26T11:51:48.336936Z","shell.execute_reply":"2023-11-26T11:51:48.336022Z","shell.execute_reply.started":"2023-11-26T11:51:47.927333Z"},"trusted":true},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import *\n","\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"spaceship-titanic\") \\\n","    .getOrCreate()\n","\n","train = spark.read.option(\"header\", True).csv(str(kaggle_tools.get_data_path()) + \"/input/spaceship-titanic/train.csv\") \\\n","            .withColumn(\"Transported\", col(\"Transported\").cast(BooleanType()))\n","train.show()"]},{"cell_type":"markdown","metadata":{},"source":["## âœ¨ Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T12:06:10.450967Z","iopub.status.busy":"2023-11-26T12:06:10.450536Z","iopub.status.idle":"2023-11-26T12:06:10.931479Z","shell.execute_reply":"2023-11-26T12:06:10.930275Z","shell.execute_reply.started":"2023-11-26T12:06:10.450933Z"},"trusted":true},"outputs":[],"source":["import mlflow\n","import shutil\n","import subprocess\n","import numpy as np\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sentence_transformers import SentenceTransformer\n","from functools import partial\n","from pyspark import keyword_only\n","from pyspark.ml import UnaryTransformer, Pipeline, Transformer, PipelineModel\n","from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, VectorSizeHint\n","from pyspark.ml.functions import array_to_vector, vector_to_array\n","from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n","from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n","from pyspark.sql.functions import udf\n","\n","\n","subprocess.Popen([\"mlflow\",\"ui\"])\n","\n","sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n","\n","model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","embeddings = model.encode(sentences)\n","\n","        \n","class StringTransformer(\n","    Transformer,\n","    HasInputCol,\n","    HasOutputCol,\n","    DefaultParamsReadable,\n","    DefaultParamsWritable,\n","):\n","    default_value = Param(\n","        Params._dummy(),\n","        \"default_value\",\n","        \"default_value\",\n","        typeConverter=TypeConverters.toString,\n","    )\n","\n","    @keyword_only\n","    def __init__(self, inputCol=None, outputCol=None, default_value=None):\n","        super(StringTransformer, self).__init__()\n","        self.default_value = Param(self, \"default_value\", \"unknown\")\n","        self._setDefault(default_value=[])\n","        kwargs = self._input_kwargs\n","        self.setParams(**kwargs)\n","\n","    @keyword_only\n","    def setParams(self, inputCol=None, outputCol=None, default_value=None):\n","        kwargs = self._input_kwargs\n","        return self._set(**kwargs)\n","\n","    def setDefaultValue(self, value):\n","        return self._set(default_value=value)\n","\n","    def getDefaultValue(self):\n","        return self.getOrDefault(self.default_value)\n","\n","    def setInputCol(self, value):\n","        \"\"\"\n","        Sets the value of :py:attr:`inputCol`.\n","        \"\"\"\n","        return self._set(inputCol=value)\n","\n","    def setOutputCol(self, value):\n","        \"\"\"\n","        Sets the value of :py:attr:`outputCol`.\n","        \"\"\"\n","        return self._set(outputCol=value)\n","\n","    def _transform(self, dataset):\n","        from pyspark.sql.functions import col, udf\n","        from pyspark.sql.types import StringType\n","        from sentence_transformers import SentenceTransformer\n","        \n","        out_col = self.getOutputCol()\n","        in_col = self.getInputCol()\n","        \n","        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","    \n","        upperCaseUDF = udf(lambda x:model.encode(str(x), show_progress_bar=False).tolist(), ArrayType(FloatType()))\n","\n","        return dataset.withColumn(\n","            out_col,\n","            upperCaseUDF(col(in_col)),\n","        )\n","    \n","        \n","class ColumnCastTransformer(\n","        Transformer, HasInputCol, HasOutputCol,\n","        DefaultParamsReadable, DefaultParamsWritable):\n","\n","    @keyword_only\n","    def __init__(self, inputCol=None, outputCol=None):\n","        super(ColumnCastTransformer, self).__init__()\n","        kwargs = self._input_kwargs\n","        self.setParams(**kwargs)\n","\n","    @keyword_only\n","    def setParams(self, inputCol=None, outputCol=None):\n","        kwargs = self._input_kwargs\n","        return self._set(**kwargs)\n","\n","    # Required in Spark >= 3.0\n","    def setInputCol(self, value):\n","        \"\"\"\n","        Sets the value of :py:attr:`inputCol`.\n","        \"\"\"\n","        return self._set(inputCol=value)\n","\n","    # Required in Spark >= 3.0\n","    def setOutputCol(self, value):\n","        \"\"\"\n","        Sets the value of :py:attr:`outputCol`.\n","        \"\"\"\n","        return self._set(outputCol=value)\n","\n","    def _transform(self, dataset):\n","        from pyspark.sql.functions import lit\n","\n","        if \"Transported\" not in dataset.columns:\n","            dataset = dataset.withColumn(\"Transported\", lit(None).cast(BooleanType()))\n","\n","        return dataset.withColumn(\"CryoSleep\", dataset[\"CryoSleep\"].cast(BooleanType())) \\\n","            .withColumn(\"VIP\", dataset[\"VIP\"].cast(BooleanType())) \\\n","            .withColumn(\"Age\", dataset[\"Age\"].cast(DoubleType())) \\\n","            .withColumn(\"RoomService\", dataset[\"RoomService\"].cast(DoubleType())) \\\n","            .withColumn(\"FoodCourt\", dataset[\"FoodCourt\"].cast(DoubleType())) \\\n","            .withColumn(\"ShoppingMall\", dataset[\"ShoppingMall\"].cast(DoubleType())) \\\n","            .withColumn(\"Spa\", dataset[\"Spa\"].cast(DoubleType())) \\\n","            .withColumn(\"VRDeck\", dataset[\"VRDeck\"].cast(DoubleType()))\n","\n","\n","class ListToArrayTransformer(\n","        Transformer, HasInputCol, HasOutputCol,\n","        DefaultParamsReadable, DefaultParamsWritable):\n","\n","    @keyword_only\n","    def __init__(self, inputCol=None, outputCol=None):\n","        super(ListToArrayTransformer, self).__init__()\n","        kwargs = self._input_kwargs\n","        self.setParams(**kwargs)\n","\n","    @keyword_only\n","    def setParams(self, inputCol=None, outputCol=None):\n","        kwargs = self._input_kwargs\n","        return self._set(**kwargs)\n","\n","    # Required in Spark >= 3.0\n","    def setInputCol(self, value):\n","        \"\"\"\n","        Sets the value of :py:attr:`inputCol`.\n","        \"\"\"\n","        return self._set(inputCol=value)\n","\n","    # Required in Spark >= 3.0\n","    def setOutputCol(self, value):\n","        \"\"\"\n","        Sets the value of :py:attr:`outputCol`.\n","        \"\"\"\n","        return self._set(outputCol=value)\n","\n","    def _transform(self, dataset):\n","        out_col = self.getOutputCol()\n","        return dataset.withColumn(out_col, array_to_vector(self.getInputCol()))\n","    \n","cct = ColumnCastTransformer()\n","    \n","homeplanetIndexer = StringIndexer(inputCol=\"HomePlanet\", outputCol=\"HomePlanet_processed\",\n","    stringOrderType=\"frequencyDesc\", handleInvalid=\"keep\")\n","destinationIndexer = StringIndexer(inputCol=\"Destination\", outputCol=\"Destination_processed\",\n","    stringOrderType=\"frequencyDesc\", handleInvalid=\"keep\")\n","ohe_homeplanet = OneHotEncoder().setInputCol(\"HomePlanet_processed\").setOutputCol(\"HomePlanet_onehot\")\n","ohe_destination = OneHotEncoder().setInputCol(\"Destination_processed\").setOutputCol(\"Destination_onehot\")\n","\n","name_ = StringTransformer().setInputCol('Name').setOutputCol(\"Name_Processed\")\n","cabin_ = StringTransformer().setInputCol('Cabin').setOutputCol(\"Cabin_Processed\")\n","\n","sizeHintName = VectorSizeHint(inputCol=\"Name_Processed_vec\", size=384)\n","sizeHintCabin = VectorSizeHint(inputCol=\"Cabin_Processed_vec\", size=384)\n","\n","name_vec_ = ListToArrayTransformer().setInputCol('Name_Processed').setOutputCol(\"Name_Processed_vec\")\n","cabin_vec_ = ListToArrayTransformer().setInputCol('Cabin_Processed').setOutputCol(\"Cabin_Processed_vec\")\n","\n","vecAssembler = VectorAssembler(handleInvalid=\"keep\", outputCol=\"features\")\n","vecAssembler.setInputCols([\"Name_Processed_vec\",\n","                           \"Cabin_Processed_vec\",\n","                           \"CryoSleep\",\n","                           \"HomePlanet_onehot\",\n","                           \"Destination_onehot\",\n","                           \"Age\",\n","                           \"VIP\",\n","                           \"RoomService\",\n","                           \"FoodCourt\",\n","                           \"ShoppingMall\",\n","                           \"Spa\",\n","                           \"VRDeck\",\n","                          ])\n","\n","pipeline = Pipeline(stages=[cct, name_, cabin_, homeplanetIndexer, destinationIndexer, ohe_homeplanet, ohe_destination, \n","                            name_vec_, cabin_vec_, sizeHintName, sizeHintCabin, vecAssembler])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T12:06:40.103682Z","iopub.status.busy":"2023-11-26T12:06:40.103253Z","iopub.status.idle":"2023-11-26T12:06:40.175699Z","shell.execute_reply":"2023-11-26T12:06:40.174308Z","shell.execute_reply.started":"2023-11-26T12:06:40.103647Z"},"trusted":true},"outputs":[],"source":["try:\n","    shutil.rmtree(str(kaggle_tools.get_data_path()) + \"/working/pipeline\")\n","except:\n","    pass\n","\n","pipelineModel = pipeline.fit(train)\n","pipelineModel.save(str(kaggle_tools.get_data_path()) + \"/working/pipeline\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    mlflow.create_experiment(\"titanic\")\n","except:\n","    pass\n","mlflow.set_experiment(\"titanic\")\n","\n","train = spark.read.option(\"header\", True).csv(str(kaggle_tools.get_data_path()) + \"/input/spaceship-titanic/train.csv\") \\\n","            .withColumn(\"Transported\", col(\"Transported\").cast(BooleanType()))\n","test = spark.read.option(\"header\", True).csv(str(kaggle_tools.get_data_path()) + \"/input/spaceship-titanic/test.csv\")\n","sample_submission = spark.read.option(\"header\", True).csv(str(kaggle_tools.get_data_path()) + \"/input/spaceship-titanic/sample_submission.csv\")\n","\n","pipelineModel = PipelineModel.load(str(kaggle_tools.get_data_path()) + \"/working/pipeline\")\n","\n","try:\n","    shutil.rmtree(str(kaggle_tools.get_data_path()) + \"/working/train_data\")\n","except:\n","    pass\n","processed_train_df = pipelineModel.transform(train)\n","processed_train_df.write.parquet(\"working/train_data\")\n","\n","try:\n","    shutil.rmtree(str(kaggle_tools.get_data_path()) + \"/working/test_data\")\n","except:\n","    pass\n","processed_test_df = pipelineModel.transform(test)\n","processed_test_df.write.parquet(\"working/test_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T12:06:41.564278Z","iopub.status.busy":"2023-11-26T12:06:41.5635Z","iopub.status.idle":"2023-11-26T12:06:41.593753Z","shell.execute_reply":"2023-11-26T12:06:41.592592Z","shell.execute_reply.started":"2023-11-26T12:06:41.564237Z"},"trusted":true},"outputs":[],"source":["# Read transformed data from files\n","processed_train_df = spark.read.parquet(\"working/train_data\")\n","processed_test_df = spark.read.parquet(\"working/test_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feats_test = processed_test_df.select(\"features\", \"Transported\").collect()\n","feats_train = processed_train_df.select(\"features\", \"Transported\").collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_and_val_transformed = np.stack([feats_train[x][\"features\"].toArray() for x in range(len(feats_train))])\n","X_test = np.stack([feats_test[x][\"features\"].toArray() for x in range(len(feats_test))])\n","y_train_and_val = np.stack([feats_train[x][\"Transported\"] for x in range(len(feats_train))])"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from hyperopt import fmin, tpe, hp, anneal, Trials\n","from sklearn.model_selection import KFold, cross_val_score\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_train_and_val_transformed, y_train_and_val, test_size=0.1, random_state=42)\n","\n","random_state = 42\n","n_iter = 2\n","num_folds = 5\n","kf = KFold(n_splits=num_folds, random_state=random_state, shuffle=True)\n","\n","def gb_acc_cv(params, random_state=random_state, cv=kf, X=X_train, y=y_train):\n","    # the function gets a set of variable parameters in \"param\"\n","    params = {'n_estimators': int(params['n_estimators']), \n","            'max_depth': int(params['max_depth']), \n","            'learning_rate': params['learning_rate']}\n","    \n","    # we use this params to create a new LGBM Regressor\n","    model = XGBClassifier(random_state=random_state, **params)\n","    \n","    # and then conduct the cross validation with the same folds as before\n","    score = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1).mean()\n","\n","    return score\n","\n","# possible values of parameters\n","space={'n_estimators': hp.quniform('n_estimators', 10, 500, 1),\n","       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n","       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n","      }\n","\n","# trials will contain logging information\n","trials = Trials()\n","\n","best=fmin(fn=gb_acc_cv, # function to optimize\n","          space=space, \n","          algo=anneal.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n","          max_evals=n_iter, # maximum number of iterations\n","          trials=trials, # logging\n","          rstate=np.random.default_rng(random_state) # fixing random state for the reproducibility\n","         )\n","mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n","with mlflow.start_run():\n","    dataset_train = mlflow.data.from_numpy(np.hstack([X_train, np.expand_dims(y_train, 1)]))\n","    dataset_val = mlflow.data.from_numpy(np.hstack([X_val, np.expand_dims(y_val, 1)]))\n","    # computing the score on the test set\n","    model = XGBClassifier(random_state=random_state, n_estimators=int(best['n_estimators']),\n","                        max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\n","    model.fit(X_train, y_train)\n","    sa_test_score=accuracy_score(y_val, model.predict(X_val))\n","    mlflow.xgboost.log_model(model, artifact_path=\"model\")\n","    mlflow.log_params(model.get_params())\n","    mlflow.log_metric(\"accuracy\", sa_test_score)\n","    mlflow.log_input(dataset_train, context=\"training\")\n","    mlflow.log_input(dataset_val, context=\"validation\")\n","\n","print(\"Best Acc {:.3f} params {}\".format(gb_acc_cv(best), best))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = XGBClassifier(random_state=random_state, n_estimators=int(best['n_estimators']),\n","                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\n","model.fit(X_train_and_val_transformed, y_train_and_val)\n","preds = model.predict(X_test)\n","preds"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":3220602,"sourceId":34377,"sourceType":"competition"},{"datasetId":4062029,"sourceId":7056845,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
